{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Literal\n",
    "from tqdm import tqdm, trange\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((586672, 15), (586672, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"processed_database_2.csv\", low_memory=False)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data.drop([], axis=1)\n",
    "\n",
    "X = data.iloc[:, 1:].to_numpy()\n",
    "Y = data.iloc[:, :1].to_numpy()\n",
    "\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train/Test Split\n",
    "#X = X.sample(n=1000).reset_index(drop=True)\n",
    "\n",
    "idxs = np.random.choice(np.arange(X.shape[0]), int(0.8*X.shape[0]), replace=False)\n",
    "\n",
    "X_train = X[idxs]\n",
    "Y_train = Y[idxs]\n",
    "\n",
    "X_test = X[~idxs]\n",
    "Y_test = Y[~idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Xavier_init(n_pre, n_post):\n",
    "    w0 = np.sqrt(6 / (n_pre + n_post))\n",
    "    W = np.random.uniform(-w0, w0, (n_post, n_pre))\n",
    "    b = np.random.uniform(-w0, w0, (n_post, 1))\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(n_x: int, n_h: tuple, n_y: int):\n",
    "    W1, b1 = Xavier_init(n_x, n_h[0])\n",
    "    W2, b2 = Xavier_init(n_h[0], n_h[1])\n",
    "    W3, b3 = Xavier_init(n_h[1], n_y)\n",
    "\n",
    "    We = {\"W1\": W1, \"W2\": W2, \"W3\": W3, \"b1\": b1, \"b2\": b2, \"b3\": b3}\n",
    "    return We"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def mse(Y_true: np.ndarray, Y_pred: np.ndarray):\n",
    "    return ((Y_pred - Y_true) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def relu(X: np.ndarray):\n",
    "    A = X * (X > 0)\n",
    "    dA = 1 * (X > 0)\n",
    "    return A, dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def linear(X: np.ndarray):\n",
    "    A = X\n",
    "    dA = 1\n",
    "    return A, dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(We, X):\n",
    "    W1 = We[\"W1\"]\n",
    "    W2 = We[\"W2\"]\n",
    "    W3 = We[\"W3\"]\n",
    "    b1 = We[\"b1\"]\n",
    "    b2 = We[\"b2\"]\n",
    "    b3 = We[\"b3\"]\n",
    "    \n",
    "    Z1 = np.dot(W1, X.T) + b1\n",
    "    A1, dA1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2, dA2 = relu(Z2)\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3, dA3 = linear(Z3)\n",
    "    \n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"dA1\": dA1, \"Z2\": Z2, \"A2\": A2, \"dA2\": dA2, \"Z3\": Z3, \"A3\": A3, \"dA3\": dA3}\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradients(X: np.ndarray, Y: np.ndarray, We: dict):\n",
    "    N = X.shape[0]\n",
    "    W1 = We[\"W1\"]\n",
    "    W2 = We[\"W2\"]\n",
    "    W3 = We[\"W3\"]\n",
    "    b1 = We[\"b1\"]\n",
    "    b2 = We[\"b2\"]\n",
    "    b3 = We[\"b3\"]\n",
    "\n",
    "    # Forward pass\n",
    "    cache = forward_propagate(We, X)\n",
    "\n",
    "    # Calculate loss\n",
    "    J = mse(Y.T, cache[\"A3\"])\n",
    "\n",
    "    # Backward pass\n",
    "    dZ3 = 2*(cache[\"A3\"] - Y.T) * cache[\"dA3\"]\n",
    "    dW3 = 1/N * np.dot(dZ3, cache[\"A2\"].T)\n",
    "    db3 = 1/N * np.sum(dZ3, axis=1, keepdims=True)\n",
    "\n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = dA2 * cache[\"dA2\"]  # Derivative of ReLU\n",
    "    dW2 = 1/N * np.dot(dZ2, cache[\"A1\"].T)\n",
    "    db2 = 1/N * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = dA1 * cache[\"dA1\"]  # Derivative of ReLU\n",
    "    dW1 = 1/N * np.dot(dZ1, X)\n",
    "    db1 = 1/N * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    # Return gradients and loss\n",
    "    dWe = {\"dW1\": dW1, \"dW2\": dW2, \"dW3\": dW3, \"db1\": db1, \"db2\": db2, \"db3\": db3}\n",
    "    return J, dWe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(We, dWe, learning_rate):\n",
    "    W1 = We[\"W1\"]\n",
    "    W2 = We[\"W2\"]\n",
    "    W3 = We[\"W3\"]\n",
    "    b1 = We[\"b1\"]\n",
    "    b2 = We[\"b2\"]\n",
    "    b3 = We[\"b3\"]\n",
    "\n",
    "    dW1 = dWe[\"dW1\"] * learning_rate\n",
    "    dW2 = dWe[\"dW2\"] * learning_rate\n",
    "    dW3 = dWe[\"dW3\"] * learning_rate\n",
    "    db1 = dWe[\"db1\"] * learning_rate\n",
    "    db2 = dWe[\"db2\"] * learning_rate\n",
    "    db3 = dWe[\"db3\"] * learning_rate\n",
    "\n",
    "    W1 -= dW1\n",
    "    W2 -= dW2\n",
    "    W3 -= dW3\n",
    "    b1 -= db1\n",
    "    b2 -= db2\n",
    "    b3 -= db3\n",
    "\n",
    "    We = {\"W1\": W1, \"W2\": W2, \"W3\": W3, \"b1\": b1, \"b2\": b2, \"b3\": b3}\n",
    "    return We"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_momentum(We, dWe, mWe, learning_rate, momentum_rate):\n",
    "    W1 = We[\"W1\"]\n",
    "    W2 = We[\"W2\"]\n",
    "    W3 = We[\"W3\"]\n",
    "    b1 = We[\"b1\"]\n",
    "    b2 = We[\"b2\"]\n",
    "    b3 = We[\"b3\"]\n",
    "\n",
    "    dW1 = dWe[\"dW1\"] * learning_rate + mWe[\"mW1\"] * momentum_rate\n",
    "    dW2 = dWe[\"dW2\"] * learning_rate + mWe[\"mW2\"] * momentum_rate\n",
    "    dW3 = dWe[\"dW3\"] * learning_rate + mWe[\"mW3\"] * momentum_rate\n",
    "    db1 = dWe[\"db1\"] * learning_rate + mWe[\"mb1\"] * momentum_rate\n",
    "    db2 = dWe[\"db2\"] * learning_rate + mWe[\"mb2\"] * momentum_rate\n",
    "    db3 = dWe[\"db3\"] * learning_rate + mWe[\"mb3\"] * momentum_rate\n",
    "\n",
    "    W1 -= dW1\n",
    "    W2 -= dW2\n",
    "    W3 -= dW3\n",
    "    b1 -= db1\n",
    "    b2 -= db2\n",
    "    b3 -= db3\n",
    "\n",
    "    We = {\"W1\": W1, \"W2\": W2, \"W3\": W3, \"b1\": b1, \"b2\": b2, \"b3\": b3}\n",
    "    mWe = {\"mW1\": dW1, \"mW2\": dW2, \"mW3\": dW3, \"mb1\": db1, \"mb2\": db2, \"mb3\": db3}\n",
    "    return We, mWe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_adagrad(We, dWe, mWe, learning_rate, epsilon):\n",
    "    W1 = We[\"W1\"]\n",
    "    W2 = We[\"W2\"]\n",
    "    W3 = We[\"W3\"]\n",
    "    b1 = We[\"b1\"]\n",
    "    b2 = We[\"b2\"]\n",
    "    b3 = We[\"b3\"]\n",
    "\n",
    "    dW1 = mWe[\"mW1\"] + np.square(dWe[\"dW1\"])\n",
    "    dW2 = mWe[\"mW2\"] + np.square(dWe[\"dW2\"])\n",
    "    dW3 = mWe[\"mW3\"] + np.square(dWe[\"dW3\"])\n",
    "    db1 = mWe[\"mb1\"] + np.square(dWe[\"db1\"])\n",
    "    db2 = mWe[\"mb2\"] + np.square(dWe[\"db2\"])\n",
    "    db3 = mWe[\"mb3\"] + np.square(dWe[\"db3\"])\n",
    "\n",
    "    W1 -= (learning_rate / (np.sqrt(dW1) + epsilon)) * dWe[\"dW1\"]\n",
    "    W2 -= (learning_rate / (np.sqrt(dW2) + epsilon)) * dWe[\"dW2\"]\n",
    "    W3 -= (learning_rate / (np.sqrt(dW3) + epsilon)) * dWe[\"dW3\"]\n",
    "    b1 -= (learning_rate / (np.sqrt(db1) + epsilon)) * dWe[\"db1\"]\n",
    "    b2 -= (learning_rate / (np.sqrt(db2) + epsilon)) * dWe[\"db2\"]\n",
    "    b3 -= (learning_rate / (np.sqrt(db3) + epsilon)) * dWe[\"db3\"]\n",
    "\n",
    "    We = {\"W1\": W1, \"W2\": W2, \"W3\": W3, \"b1\": b1, \"b2\": b2, \"b3\": b3}\n",
    "    mWe = {\"mW1\": dW1, \"mW2\": dW2, \"mW3\": dW3, \"mb1\": db1, \"mb2\": db2, \"mb3\": db3}\n",
    "    return We, mWe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_rmsprop(We, dWe, mWe, learning_rate, epsilon, rho):\n",
    "    W1 = We[\"W1\"]\n",
    "    W2 = We[\"W2\"]\n",
    "    W3 = We[\"W3\"]\n",
    "    b1 = We[\"b1\"]\n",
    "    b2 = We[\"b2\"]\n",
    "    b3 = We[\"b3\"]\n",
    "\n",
    "    dW1 = (rho * mWe[\"mW1\"]) + ((1 - rho) * np.square(dWe[\"dW1\"]))\n",
    "    dW2 = (rho * mWe[\"mW2\"]) + ((1 - rho) * np.square(dWe[\"dW2\"]))\n",
    "    dW3 = (rho * mWe[\"mW3\"]) + ((1 - rho) * np.square(dWe[\"dW3\"]))\n",
    "    db1 = (rho * mWe[\"mb1\"]) + ((1 - rho) * np.square(dWe[\"db1\"]))\n",
    "    db2 = (rho * mWe[\"mb2\"]) + ((1 - rho) * np.square(dWe[\"db2\"]))\n",
    "    db3 = (rho * mWe[\"mb3\"]) + ((1 - rho) * np.square(dWe[\"db3\"]))\n",
    "\n",
    "    W1 -= (learning_rate / (np.sqrt(dW1) + epsilon)) * dWe[\"dW1\"]\n",
    "    W2 -= (learning_rate / (np.sqrt(dW2) + epsilon)) * dWe[\"dW2\"]\n",
    "    W3 -= (learning_rate / (np.sqrt(dW3) + epsilon)) * dWe[\"dW3\"]\n",
    "    b1 -= (learning_rate / (np.sqrt(db1) + epsilon)) * dWe[\"db1\"]\n",
    "    b2 -= (learning_rate / (np.sqrt(db2) + epsilon)) * dWe[\"db2\"]\n",
    "    b3 -= (learning_rate / (np.sqrt(db3) + epsilon)) * dWe[\"db3\"]\n",
    "\n",
    "    We = {\"W1\": W1, \"W2\": W2, \"W3\": W3, \"b1\": b1, \"b2\": b2, \"b3\": b3}\n",
    "    mWe = {\"mW1\": dW1, \"mW2\": dW2, \"mW3\": dW3, \"mb1\": db1, \"mb2\": db2, \"mb3\": db3}\n",
    "    return We, mWe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_adam(We, dWe, mWe, vWe, learning_rate, epsilon, rho1, rho2):\n",
    "    W1 = We[\"W1\"]\n",
    "    W2 = We[\"W2\"]\n",
    "    W3 = We[\"W3\"]\n",
    "    b1 = We[\"b1\"]\n",
    "    b2 = We[\"b2\"]\n",
    "    b3 = We[\"b3\"]\n",
    "\n",
    "    mW1 = (rho1 * mWe[\"mW1\"]) + ((1 - rho1) * dWe[\"dW1\"])\n",
    "    mW2 = (rho1 * mWe[\"mW2\"]) + ((1 - rho1) * dWe[\"dW2\"])\n",
    "    mW3 = (rho1 * mWe[\"mW3\"]) + ((1 - rho1) * dWe[\"dW3\"])\n",
    "    mb1 = (rho1 * mWe[\"mb1\"]) + ((1 - rho1) * dWe[\"db1\"])\n",
    "    mb2 = (rho1 * mWe[\"mb2\"]) + ((1 - rho1) * dWe[\"db2\"])\n",
    "    mb3 = (rho1 * mWe[\"mb3\"]) + ((1 - rho1) * dWe[\"db3\"])\n",
    "\n",
    "    vW1 = (rho2 * vWe[\"vW1\"]) + ((1 - rho2) * np.square(dWe[\"dW1\"]))\n",
    "    vW2 = (rho2 * vWe[\"vW2\"]) + ((1 - rho2) * np.square(dWe[\"dW2\"]))\n",
    "    vW3 = (rho2 * vWe[\"vW3\"]) + ((1 - rho2) * np.square(dWe[\"dW3\"]))\n",
    "    vb1 = (rho2 * vWe[\"vb1\"]) + ((1 - rho2) * np.square(dWe[\"db1\"]))\n",
    "    vb2 = (rho2 * vWe[\"vb2\"]) + ((1 - rho2) * np.square(dWe[\"db2\"]))\n",
    "    vb3 = (rho2 * vWe[\"vb3\"]) + ((1 - rho2) * np.square(dWe[\"db3\"]))\n",
    "\n",
    "    mW1_hat = mW1 / (1 - rho1)\n",
    "    mW2_hat = mW2 / (1 - rho1)\n",
    "    mW3_hat = mW3 / (1 - rho1)\n",
    "    mb1_hat = mb1 / (1 - rho1)\n",
    "    mb2_hat = mb2 / (1 - rho1)\n",
    "    mb3_hat = mb3 / (1 - rho1)\n",
    "\n",
    "    vW1_hat = vW1 / (1 - rho2)\n",
    "    vW2_hat = vW2 / (1 - rho2)\n",
    "    vW3_hat = vW3 / (1 - rho2)\n",
    "    vb1_hat = vb1 / (1 - rho2)\n",
    "    vb2_hat = vb2 / (1 - rho2)\n",
    "    vb3_hat = vb3 / (1 - rho2)\n",
    "\n",
    "    W1 -= learning_rate * (mW1_hat / (np.sqrt(vW1_hat) + epsilon))\n",
    "    W2 -= learning_rate * (mW2_hat / (np.sqrt(vW2_hat) + epsilon))\n",
    "    W3 -= learning_rate * (mW3_hat / (np.sqrt(vW3_hat) + epsilon))\n",
    "    b1 -= learning_rate * (mb1_hat / (np.sqrt(vb1_hat) + epsilon))\n",
    "    b2 -= learning_rate * (mb2_hat / (np.sqrt(vb2_hat) + epsilon))\n",
    "    b3 -= learning_rate * (mb3_hat / (np.sqrt(vb3_hat) + epsilon))\n",
    "\n",
    "    We = {\"W1\": W1, \"W2\": W2, \"W3\": W3, \"b1\": b1, \"b2\": b2, \"b3\": b3}\n",
    "    mWe = {\"mW1\": mW1, \"mW2\": mW2, \"mW3\": mW3, \"mb1\": mb1, \"mb2\": mb2, \"mb3\": mb3}\n",
    "    vWe = {\"vW1\": vW1, \"vW2\": vW2, \"vW3\": vW3, \"vb1\": vb1, \"vb2\": vb2, \"vb3\": vb3}\n",
    "    return We, mWe, vWe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop\n",
    "def train(\n",
    "    X: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    We: dict,\n",
    "    num_epochs: int,\n",
    "    learning_rate: float,\n",
    "    batch_size: int = 200,\n",
    "    validation_split: float = 0.1,\n",
    "    optimizer: Literal[\"\", \"none\", \"momentum\", \"adagrad\", \"rmsprop\", \"adam\"] = \"none\",\n",
    "    momentum_rate: float = 0.9,\n",
    "    epsilon: float = 1e-8,\n",
    "    rho1: float = 0.9,\n",
    "    rho2: float = 0.99,\n",
    "    patience: int = 10,\n",
    "    min_delta: float = 1e-3,\n",
    "):\n",
    "    patience_counter = 0\n",
    "    best_valid_cost = np.inf\n",
    "    mWe = {\n",
    "        \"mW1\": 0,\n",
    "        \"mW2\": 0,\n",
    "        \"mW3\": 0,\n",
    "        \"mb1\": 0,\n",
    "        \"mb2\": 0,\n",
    "        \"mb3\": 0,\n",
    "    }\n",
    "    vWe = {\n",
    "        \"vW1\": 0,\n",
    "        \"vW2\": 0,\n",
    "        \"vW3\": 0,\n",
    "        \"vb1\": 0,\n",
    "        \"vb2\": 0,\n",
    "        \"vb3\": 0,\n",
    "    }\n",
    "    if validation_split is None:\n",
    "        X_train = X\n",
    "        Y_train = Y\n",
    "    else:\n",
    "        random_idxs = np.random.choice(X.shape[0], (X.shape[0]))\n",
    "        X = X[random_idxs]\n",
    "        Y = Y[random_idxs]\n",
    "        valid_start = int(X.shape[0] * validation_split)\n",
    "        X_train = X[valid_start:]\n",
    "        Y_train = Y[valid_start:]\n",
    "        X_valid = X[:valid_start]\n",
    "        Y_valid = Y[:valid_start]\n",
    "    N = X_train.shape[0]\n",
    "    if batch_size is None:\n",
    "        batch_size = X_train.shape[0]\n",
    "        mini_batch_count = 1\n",
    "    elif isinstance(batch_size, float) and batch_size < 1:\n",
    "        batch_size = int(N * batch_size)\n",
    "        mini_batch_count = N // batch_size\n",
    "    else:\n",
    "        mini_batch_count = N // batch_size\n",
    "\n",
    "    cache_train = forward_propagate(We, X_train[:batch_size])\n",
    "    cost_train = mse(Y_train[:batch_size].T, cache_train[\"A3\"])\n",
    "    costs_train = [cost_train]\n",
    "\n",
    "    cache_valid = forward_propagate(We, X_valid[: (batch_size // 10)])\n",
    "    cost_valid = mse(Y_valid[: (batch_size // 10)].T, cache_valid[\"A3\"])\n",
    "    costs_valid = [cost_valid]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cost_train_total = 0\n",
    "        cost_valid_total = 0\n",
    "\n",
    "        mini_batch_start = 0\n",
    "        mini_batch_end = batch_size\n",
    "\n",
    "        pbar = trange(\n",
    "            mini_batch_count,\n",
    "            desc=f\"Epoch {epoch+1:4d}/{num_epochs}\",\n",
    "            ncols=130,\n",
    "            leave=True,\n",
    "        )\n",
    "        for _ in pbar:\n",
    "            mini_batch_x = X_train[mini_batch_start:mini_batch_end]\n",
    "            mini_batch_y = Y_train[mini_batch_start:mini_batch_end]\n",
    "\n",
    "            mini_batch_x_valid = X_valid[mini_batch_start // 10 : mini_batch_end // 10]\n",
    "            mini_batch_y_valid = Y_valid[mini_batch_start // 10 : mini_batch_end // 10]\n",
    "\n",
    "            cost_train, dWe = calculate_gradients(mini_batch_x, mini_batch_y, We)\n",
    "            cost_train_total += cost_train\n",
    "            match optimizer:\n",
    "                case [\"\" | \"none\"]:\n",
    "                    We = update_weights(We, dWe, learning_rate)\n",
    "                case \"momentum\":\n",
    "                    We, mWe = update_weights_momentum(\n",
    "                        We, dWe, mWe, learning_rate, momentum_rate\n",
    "                    )\n",
    "                case \"adagrad\":\n",
    "                    We, mWe = update_weights_adagrad(\n",
    "                        We, dWe, mWe, learning_rate, epsilon\n",
    "                    )\n",
    "                case \"rmsprop\":\n",
    "                    We, mWe = update_weights_rmsprop(\n",
    "                        We, dWe, mWe, learning_rate, epsilon, rho1\n",
    "                    )\n",
    "                case \"adam\":\n",
    "                    We, mWe, vWe = update_weights_adam(\n",
    "                        We, dWe, mWe, vWe, learning_rate, epsilon, rho1, rho2\n",
    "                    )\n",
    "\n",
    "            cache_valid = forward_propagate(We, mini_batch_x_valid)\n",
    "            cost_valid = mse(mini_batch_y_valid.T, cache_valid[\"A3\"])\n",
    "            cost_valid_total += cost_valid\n",
    "\n",
    "            pbar.set_postfix_str(\n",
    "                f\" Training Error: {cost_train:.5f}, Validation Error: {cost_valid:.5f}\",\n",
    "                refresh=False,\n",
    "            )\n",
    "\n",
    "            mini_batch_start = mini_batch_end\n",
    "            mini_batch_end = min(mini_batch_end + batch_size, N)\n",
    "        \n",
    "        costs_train.append(cost_train_total / mini_batch_count)\n",
    "        costs_valid.append(cost_valid_total / mini_batch_count)\n",
    "        \n",
    "        pbar.set_postfix_str(\n",
    "            f\" Training Error: {costs_train[-1]:.5f}, Validation Error: {costs_valid[-1]:.5f}\",\n",
    "            refresh=False,\n",
    "        )\n",
    "\n",
    "        if costs_valid[-1] + min_delta < best_valid_cost:\n",
    "            patience_counter = 0\n",
    "            best_valid_cost = costs_valid[-1]\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "\n",
    "        # Print loss\n",
    "        # if (epoch + 1) % 100 == 0:\n",
    "        # if validation_split is None:\n",
    "        #    print(f\"Epoch: {epoch+1}/{num_epochs} - Training Error: {round(J, 5)}\")\n",
    "        # else:\n",
    "        #    print(\n",
    "        #        f\"Epoch: {epoch+1}/{num_epochs} - Training Error: {round(J, 5)} - Validation Error: {round(cost_valid, 5)}            \", end=\"\\r\"\n",
    "        #    )\n",
    "\n",
    "    return We, costs_train, costs_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch    1/100: 100%|████████████████| 1000/1000 [00:18<00:00, 53.67it/s,  Training Error: 189.97026, Validation Error: 250.59252]\n",
      "Epoch    2/100: 100%|████████████████| 1000/1000 [00:20<00:00, 48.58it/s,  Training Error: 185.69911, Validation Error: 235.24361]\n",
      "Epoch    3/100: 100%|████████████████| 1000/1000 [00:20<00:00, 47.80it/s,  Training Error: 183.71047, Validation Error: 229.41105]\n",
      "Epoch    4/100: 100%|████████████████| 1000/1000 [00:19<00:00, 52.57it/s,  Training Error: 182.75697, Validation Error: 228.12257]\n",
      "Epoch    5/100: 100%|████████████████| 1000/1000 [00:19<00:00, 51.57it/s,  Training Error: 182.86680, Validation Error: 226.46186]\n",
      "Epoch    6/100: 100%|████████████████| 1000/1000 [00:18<00:00, 54.21it/s,  Training Error: 182.62576, Validation Error: 225.34631]\n",
      "Epoch    7/100: 100%|████████████████| 1000/1000 [00:21<00:00, 47.19it/s,  Training Error: 181.72339, Validation Error: 224.56295]\n",
      "Epoch    8/100: 100%|████████████████| 1000/1000 [00:22<00:00, 44.41it/s,  Training Error: 181.30646, Validation Error: 224.39507]\n",
      "Epoch    9/100: 100%|████████████████| 1000/1000 [00:22<00:00, 45.37it/s,  Training Error: 180.83848, Validation Error: 222.34693]\n",
      "Epoch   10/100: 100%|████████████████| 1000/1000 [00:21<00:00, 46.33it/s,  Training Error: 179.68709, Validation Error: 218.66481]\n",
      "Epoch   11/100: 100%|████████████████| 1000/1000 [00:21<00:00, 46.63it/s,  Training Error: 179.90952, Validation Error: 216.58417]\n",
      "Epoch   12/100: 100%|████████████████| 1000/1000 [00:21<00:00, 47.21it/s,  Training Error: 179.65482, Validation Error: 214.95650]\n",
      "Epoch   13/100: 100%|████████████████| 1000/1000 [00:21<00:00, 46.44it/s,  Training Error: 179.81316, Validation Error: 214.02538]\n",
      "Epoch   14/100: 100%|████████████████| 1000/1000 [00:21<00:00, 46.74it/s,  Training Error: 180.25213, Validation Error: 213.54892]\n",
      "Epoch   15/100: 100%|████████████████| 1000/1000 [00:21<00:00, 46.88it/s,  Training Error: 179.27568, Validation Error: 212.40843]\n",
      "Epoch   16/100: 100%|████████████████| 1000/1000 [00:21<00:00, 46.64it/s,  Training Error: 179.17091, Validation Error: 211.58412]\n",
      "Epoch   17/100: 100%|████████████████| 1000/1000 [00:21<00:00, 45.80it/s,  Training Error: 178.61993, Validation Error: 210.03755]\n",
      "Epoch   18/100: 100%|████████████████| 1000/1000 [00:21<00:00, 45.84it/s,  Training Error: 178.07875, Validation Error: 211.00018]\n",
      "Epoch   19/100: 100%|████████████████| 1000/1000 [00:21<00:00, 47.43it/s,  Training Error: 177.51657, Validation Error: 210.96408]\n",
      "Epoch   20/100: 100%|████████████████| 1000/1000 [00:22<00:00, 45.19it/s,  Training Error: 176.25964, Validation Error: 209.37899]\n",
      "Epoch   21/100: 100%|████████████████| 1000/1000 [00:22<00:00, 45.15it/s,  Training Error: 176.34518, Validation Error: 208.01623]\n",
      "Epoch   22/100: 100%|████████████████| 1000/1000 [00:22<00:00, 44.18it/s,  Training Error: 175.59382, Validation Error: 208.87456]\n",
      "Epoch   23/100: 100%|████████████████| 1000/1000 [00:21<00:00, 46.31it/s,  Training Error: 175.14233, Validation Error: 207.32137]\n",
      "Epoch   24/100: 100%|████████████████| 1000/1000 [00:21<00:00, 46.10it/s,  Training Error: 174.10465, Validation Error: 207.51964]\n",
      "Epoch   25/100: 100%|████████████████| 1000/1000 [00:22<00:00, 44.99it/s,  Training Error: 173.99478, Validation Error: 208.43420]\n",
      "Epoch   26/100: 100%|████████████████| 1000/1000 [00:21<00:00, 46.02it/s,  Training Error: 173.35374, Validation Error: 208.12790]\n",
      "Epoch   27/100: 100%|████████████████| 1000/1000 [00:21<00:00, 46.21it/s,  Training Error: 173.02948, Validation Error: 209.20117]\n",
      "Epoch   28/100: 100%|████████████████| 1000/1000 [00:22<00:00, 44.45it/s,  Training Error: 173.13149, Validation Error: 209.67073]\n",
      "Epoch   29/100: 100%|████████████████| 1000/1000 [00:22<00:00, 44.76it/s,  Training Error: 173.34323, Validation Error: 208.28905]\n",
      "Epoch   30/100: 100%|████████████████| 1000/1000 [00:21<00:00, 46.28it/s,  Training Error: 172.59264, Validation Error: 208.20553]\n",
      "Epoch   31/100: 100%|████████████████| 1000/1000 [00:21<00:00, 47.21it/s,  Training Error: 171.95262, Validation Error: 209.08111]\n",
      "Epoch   32/100: 100%|████████████████| 1000/1000 [00:22<00:00, 45.37it/s,  Training Error: 172.05398, Validation Error: 209.59682]\n",
      "Epoch   33/100: 100%|████████████████| 1000/1000 [00:21<00:00, 46.19it/s,  Training Error: 171.75653, Validation Error: 210.66560]\n",
      "Epoch   34/100: 100%|████████████████| 1000/1000 [00:20<00:00, 48.62it/s,  Training Error: 171.35760, Validation Error: 211.29340]\n",
      "Epoch   35/100: 100%|████████████████| 1000/1000 [00:21<00:00, 45.64it/s,  Training Error: 171.55690, Validation Error: 212.28375]\n",
      "Epoch   36/100: 100%|████████████████| 1000/1000 [00:22<00:00, 44.05it/s,  Training Error: 170.67673, Validation Error: 211.77054]\n",
      "Epoch   37/100: 100%|████████████████| 1000/1000 [00:21<00:00, 45.82it/s,  Training Error: 170.32809, Validation Error: 212.07645]\n",
      "Epoch   38/100: 100%|████████████████| 1000/1000 [00:21<00:00, 45.48it/s,  Training Error: 170.03737, Validation Error: 211.96295]\n",
      "Epoch   39/100: 100%|████████████████| 1000/1000 [00:21<00:00, 47.21it/s,  Training Error: 170.04098, Validation Error: 210.73655]\n",
      "Epoch   40/100: 100%|████████████████| 1000/1000 [00:21<00:00, 45.74it/s,  Training Error: 168.64788, Validation Error: 212.39539]\n",
      "Epoch   41/100: 100%|████████████████| 1000/1000 [00:21<00:00, 47.05it/s,  Training Error: 168.39526, Validation Error: 212.45588]\n",
      "Epoch   42/100: 100%|████████████████| 1000/1000 [00:21<00:00, 47.46it/s,  Training Error: 167.21333, Validation Error: 212.26939]\n",
      "Epoch   43/100: 100%|████████████████| 1000/1000 [00:21<00:00, 46.80it/s,  Training Error: 166.50636, Validation Error: 213.11540]\n",
      "Epoch   44/100: 100%|████████████████| 1000/1000 [00:21<00:00, 46.67it/s,  Training Error: 166.33807, Validation Error: 212.34014]\n",
      "Epoch   45/100: 100%|████████████████| 1000/1000 [00:22<00:00, 43.55it/s,  Training Error: 165.21124, Validation Error: 212.65608]\n",
      "Epoch   46/100: 100%|████████████████| 1000/1000 [00:22<00:00, 43.77it/s,  Training Error: 163.93461, Validation Error: 213.47922]\n",
      "Epoch   47/100: 100%|████████████████| 1000/1000 [00:22<00:00, 45.38it/s,  Training Error: 163.81770, Validation Error: 212.94852]\n",
      "Epoch   48/100: 100%|████████████████| 1000/1000 [00:22<00:00, 44.14it/s,  Training Error: 162.88436, Validation Error: 214.87794]\n",
      "Epoch   49/100: 100%|████████████████| 1000/1000 [00:21<00:00, 46.03it/s,  Training Error: 161.85345, Validation Error: 214.24398]\n",
      "Epoch   50/100: 100%|████████████████| 1000/1000 [00:22<00:00, 45.07it/s,  Training Error: 161.22773, Validation Error: 213.79436]\n",
      "Epoch   51/100: 100%|████████████████| 1000/1000 [00:21<00:00, 45.82it/s,  Training Error: 159.93099, Validation Error: 214.27197]\n",
      "Epoch   52/100: 100%|████████████████| 1000/1000 [00:21<00:00, 45.63it/s,  Training Error: 159.18066, Validation Error: 214.21983]\n",
      "Epoch   53/100: 100%|████████████████| 1000/1000 [00:21<00:00, 46.18it/s,  Training Error: 158.61612, Validation Error: 214.92241]\n",
      "Epoch   54/100: 100%|████████████████| 1000/1000 [00:21<00:00, 45.69it/s,  Training Error: 157.67093, Validation Error: 217.49995]\n",
      "Epoch   55/100: 100%|████████████████| 1000/1000 [00:21<00:00, 46.02it/s,  Training Error: 157.20938, Validation Error: 215.78314]\n",
      "Epoch   56/100: 100%|████████████████| 1000/1000 [00:21<00:00, 45.79it/s,  Training Error: 156.14862, Validation Error: 216.67834]\n",
      "Epoch   57/100: 100%|████████████████| 1000/1000 [00:22<00:00, 44.79it/s,  Training Error: 155.55610, Validation Error: 217.19261]\n",
      "Epoch   58/100: 100%|████████████████| 1000/1000 [00:21<00:00, 46.21it/s,  Training Error: 154.48396, Validation Error: 216.75535]\n",
      "Epoch   59/100: 100%|████████████████| 1000/1000 [00:22<00:00, 44.88it/s,  Training Error: 153.63810, Validation Error: 215.71063]\n",
      "Epoch   60/100: 100%|████████████████| 1000/1000 [00:22<00:00, 45.33it/s,  Training Error: 152.61684, Validation Error: 214.28109]\n",
      "Epoch   61/100: 100%|████████████████| 1000/1000 [00:20<00:00, 47.64it/s,  Training Error: 152.26333, Validation Error: 214.17651]\n",
      "Epoch   62/100: 100%|████████████████| 1000/1000 [00:21<00:00, 45.95it/s,  Training Error: 151.42609, Validation Error: 213.99406]\n",
      "Epoch   63/100: 100%|████████████████| 1000/1000 [00:21<00:00, 45.73it/s,  Training Error: 150.72884, Validation Error: 213.23791]\n",
      "Epoch   64/100: 100%|████████████████| 1000/1000 [00:36<00:00, 27.46it/s,  Training Error: 150.76846, Validation Error: 213.76887]\n",
      "Epoch   65/100: 100%|████████████████| 1000/1000 [00:22<00:00, 44.91it/s,  Training Error: 149.41988, Validation Error: 213.39932]\n",
      "Epoch   66/100: 100%|████████████████| 1000/1000 [00:22<00:00, 44.75it/s,  Training Error: 148.99446, Validation Error: 212.52245]\n",
      "Epoch   67/100: 100%|████████████████| 1000/1000 [00:22<00:00, 43.73it/s,  Training Error: 147.75181, Validation Error: 212.26110]\n",
      "Epoch   68/100: 100%|████████████████| 1000/1000 [00:21<00:00, 46.03it/s,  Training Error: 147.64261, Validation Error: 212.42115]\n",
      "Epoch   69/100: 100%|████████████████| 1000/1000 [00:21<00:00, 47.21it/s,  Training Error: 147.58789, Validation Error: 213.73816]\n",
      "Epoch   70/100: 100%|████████████████| 1000/1000 [00:23<00:00, 43.06it/s,  Training Error: 147.03779, Validation Error: 211.21299]\n",
      "Epoch   71/100: 100%|████████████████| 1000/1000 [00:24<00:00, 41.02it/s,  Training Error: 147.13819, Validation Error: 212.70282]\n",
      "Epoch   72/100: 100%|████████████████| 1000/1000 [00:23<00:00, 42.17it/s,  Training Error: 146.72289, Validation Error: 214.62115]\n",
      "Epoch   73/100: 100%|████████████████| 1000/1000 [00:23<00:00, 42.03it/s,  Training Error: 145.82781, Validation Error: 213.69023]\n",
      "Epoch   74/100: 100%|████████████████| 1000/1000 [00:23<00:00, 41.81it/s,  Training Error: 144.61675, Validation Error: 213.12115]\n",
      "Epoch   75/100: 100%|████████████████| 1000/1000 [00:23<00:00, 41.92it/s,  Training Error: 143.85301, Validation Error: 214.35542]\n",
      "Epoch   76/100: 100%|████████████████| 1000/1000 [00:23<00:00, 42.97it/s,  Training Error: 143.53199, Validation Error: 214.44421]\n",
      "Epoch   77/100: 100%|████████████████| 1000/1000 [00:23<00:00, 43.44it/s,  Training Error: 143.08566, Validation Error: 216.44527]\n",
      "Epoch   78/100: 100%|████████████████| 1000/1000 [00:22<00:00, 43.62it/s,  Training Error: 141.62736, Validation Error: 215.51599]\n",
      "Epoch   79/100: 100%|████████████████| 1000/1000 [00:23<00:00, 42.68it/s,  Training Error: 141.48576, Validation Error: 215.65476]\n",
      "Epoch   80/100: 100%|████████████████| 1000/1000 [00:23<00:00, 41.95it/s,  Training Error: 141.53428, Validation Error: 216.48347]\n",
      "Epoch   81/100: 100%|████████████████| 1000/1000 [00:24<00:00, 40.79it/s,  Training Error: 140.45440, Validation Error: 217.09713]\n",
      "Epoch   82/100: 100%|████████████████| 1000/1000 [00:24<00:00, 40.92it/s,  Training Error: 139.90165, Validation Error: 217.74453]\n",
      "Epoch   83/100: 100%|████████████████| 1000/1000 [00:24<00:00, 41.03it/s,  Training Error: 138.72980, Validation Error: 220.02951]\n",
      "Epoch   84/100: 100%|████████████████| 1000/1000 [00:24<00:00, 40.13it/s,  Training Error: 138.16159, Validation Error: 219.40666]\n",
      "Epoch   85/100: 100%|████████████████| 1000/1000 [01:35<00:00, 10.47it/s,  Training Error: 137.67262, Validation Error: 219.64134]\n",
      "Epoch   86/100: 100%|████████████████| 1000/1000 [00:24<00:00, 41.39it/s,  Training Error: 137.81737, Validation Error: 218.55876]\n",
      "Epoch   87/100: 100%|████████████████| 1000/1000 [00:23<00:00, 42.22it/s,  Training Error: 137.43877, Validation Error: 218.48480]\n",
      "Epoch   88/100: 100%|████████████████| 1000/1000 [01:00<00:00, 16.56it/s,  Training Error: 137.19594, Validation Error: 217.57276]\n",
      "Epoch   89/100: 100%|████████████████| 1000/1000 [00:24<00:00, 41.48it/s,  Training Error: 137.00932, Validation Error: 222.12869]\n",
      "Epoch   90/100: 100%|████████████████| 1000/1000 [00:24<00:00, 41.39it/s,  Training Error: 135.98988, Validation Error: 219.71744]\n",
      "Epoch   91/100: 100%|████████████████| 1000/1000 [00:24<00:00, 41.38it/s,  Training Error: 136.13688, Validation Error: 216.08557]\n",
      "Epoch   92/100: 100%|████████████████| 1000/1000 [00:24<00:00, 40.08it/s,  Training Error: 135.20594, Validation Error: 219.76476]\n",
      "Epoch   93/100: 100%|████████████████| 1000/1000 [00:26<00:00, 38.22it/s,  Training Error: 134.64199, Validation Error: 218.13988]\n",
      "Epoch   94/100:  24%|████▏             | 235/1000 [00:06<00:20, 37.00it/s,  Training Error: 125.98556, Validation Error: 95.52683]"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "We = initialize_weights(15, (500, 500), 1)\n",
    "We_final, costs_train, costs_valid = train(X_train, Y_train, We, num_epochs=100, batch_size=0.001, learning_rate=0.001, optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(len(costs_train)), costs_train, \"b\", label=\"Training\")\n",
    "ax.plot(np.arange(len(costs_valid)), costs_valid, \"r\", label=\"Validation\")\n",
    "ax.set_xlabel(\"Iterations\")\n",
    "ax.set_ylabel(\"Cost\")\n",
    "ax.set_title(\"Iteration vs Cost (Neural Network)\")\n",
    "\n",
    "fig.savefig(\"neural_cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_train = forward_propagate(We_final, X_train)\n",
    "Y_pred_train = cache_train[\"A3\"]\n",
    "residuals_train = Y_train.T - Y_pred_train\n",
    "train_cost = mse(Y_train.T, Y_pred_train)\n",
    "print(f\"Training cost: {np.squeeze(train_cost).round(2)}\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Y_train, Y_pred_train, (0.01))\n",
    "ax.set_xlabel(\"Actual Value\")\n",
    "ax.set_ylabel(\"Predicted Value\")\n",
    "ax.set_title(\"Actual vs Prediction (Neural Network, Train)\")\n",
    "\n",
    "ax.plot(np.arange(100), np.arange(100), \"r\")\n",
    "fig.savefig(\"neural_train_pred\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_autoscale_on(False)\n",
    "ax.scatter(Y_train, residuals_train, (0.01))\n",
    "ax.set_xbound(-50, 100)\n",
    "ax.set_ybound(-100, 100)\n",
    "ax.set_xlabel(\"Predicted Value\")\n",
    "ax.set_ylabel(\"Residual\")\n",
    "ax.set_title(\"Predicted Value vs Residual (Neural Network, Train)\")\n",
    "\n",
    "ax.plot(np.arange(150)-50, np.zeros((150,)), \"black\")\n",
    "\n",
    "fig.savefig(\"neural_train_resd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_test = forward_propagate(We_final, X_test)\n",
    "Y_pred_test = cache_test[\"A3\"]\n",
    "residuals_test = Y_test.T - Y_pred_test\n",
    "test_cost = mse(Y_test.T, Y_pred_test)\n",
    "print(f\"Training cost: {np.squeeze(test_cost).round(2)}\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Y_test, Y_pred_test, (0.01))\n",
    "ax.set_xlabel(\"Actual Value\")\n",
    "ax.set_ylabel(\"Predicted Value\")\n",
    "ax.set_title(\"Actual vs Prediction (Neural Network, Test)\")\n",
    "\n",
    "ax.plot(np.arange(100), np.arange(100), \"r\")\n",
    "fig.savefig(\"neural_test_pred\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_autoscale_on(False)\n",
    "ax.scatter(Y_pred_test, residuals_test, (0.01))\n",
    "ax.set_xbound(-50, 100)\n",
    "ax.set_ybound(-100, 100)\n",
    "ax.set_xlabel(\"Predicted Value\")\n",
    "ax.set_ylabel(\"Residual\")\n",
    "ax.set_title(\"Predicted Value vs Residual (Neural Network, Test)\")\n",
    "\n",
    "ax.plot(np.arange(150)-50, np.zeros((150,)), \"black\")\n",
    "\n",
    "fig.savefig(\"neural_test_resd\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b884cd3834b45e97c2c4fae50fd751c5823ae7720d5b9a16c16ce6076342eebe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
